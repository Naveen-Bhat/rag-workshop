# CS501: Natural Language Processing

## Course Information
- **Course Code:** CS501
- **Credits:** 4
- **Prerequisites:** CS401 (Deep Learning), MATH201 (Linear Algebra)
- **Instructor:** Dr. Priya Sharma
- **Office:** AI Research Center, Room 401
- **Email:** priya.sharma@fictional.edu
- **Semester:** Fall only
- **Level:** Graduate (open to advanced undergraduates with instructor permission)

## Course Description

This graduate-level course provides comprehensive coverage of natural language processing (NLP), from classical techniques to modern deep learning approaches. Students will learn to build systems that understand, generate, and interact with human language.

NLP is one of the most active areas of AI research, powering applications from search engines to virtual assistants to machine translation. This course covers the full spectrum of NLP, with emphasis on the transformer-based models that have revolutionized the field.

Students will implement NLP systems, work with large language models, and explore cutting-edge topics like retrieval-augmented generation (RAG), prompt engineering, and AI safety considerations in language systems.

## Learning Objectives

Upon successful completion of this course, students will be able to:
- Understand the linguistic foundations of NLP
- Implement text preprocessing and feature extraction pipelines
- Apply classical and modern NLP algorithms
- Fine-tune and prompt large language models
- Build end-to-end NLP applications
- Evaluate NLP systems rigorously
- Understand ethical considerations in language AI
- Read and understand current NLP research papers

## Topics Covered

### Module 1: Foundations (Weeks 1-2)
- Linguistic basics: syntax, semantics, pragmatics
- Text preprocessing: tokenization, normalization, stemming
- Text representation: bag-of-words, TF-IDF
- Word embeddings: Word2Vec, GloVe, FastText

### Module 2: Classical NLP (Weeks 3-4)
- Language modeling (n-grams)
- Part-of-speech tagging
- Named entity recognition
- Sentiment analysis with classical methods
- Topic modeling (LDA)

### Module 3: Neural NLP (Weeks 5-6)
- RNNs for sequence modeling
- Sequence-to-sequence with attention
- Neural machine translation
- Text classification with neural networks

### Module 4: Transformers & Pre-trained Models (Weeks 7-9)
- Transformer architecture deep dive
- BERT: pre-training and fine-tuning
- GPT family and autoregressive models
- T5, BART, and encoder-decoder models
- Efficient transformers and model compression

### Module 5: Large Language Models (Weeks 10-11)
- Scaling laws and emergent abilities
- Prompt engineering and in-context learning
- Instruction tuning and RLHF
- Retrieval-Augmented Generation (RAG)
- Tool use and agents

### Module 6: Advanced Topics (Weeks 12-13)
- Multimodal language models
- Evaluation of generative models
- Bias and fairness in NLP
- Safety and alignment considerations
- Interpretability in NLP models

### Module 7: Research Projects (Week 14)
- Project presentations
- Peer review and discussion

## Textbook

**Required:** "Speech and Language Processing" by Jurafsky and Martin (3rd Edition, available online)

**Reference:** Recent papers from ACL, EMNLP, NAACL conferences (reading list provided)

## Grading

| Component | Weight |
|-----------|--------|
| Programming Assignments (3) | 25% |
| Paper Presentations (2) | 15% |
| Midterm Exam | 15% |
| Research Project | 35% |
| Class Discussion | 10% |

## Research Project

The culminating project is a research-level investigation. Options include:
- Novel research contribution (publishable quality)
- Comprehensive survey of an NLP subfield
- Reproduction and analysis of a recent paper
- Application to a domain-specific NLP challenge

## Prerequisites in Detail

- **CS401 (Deep Learning):** Essential for understanding neural NLP methods
- **MATH201 (Linear Algebra):** Needed for understanding embeddings and transformers

Students should be comfortable with:
- PyTorch or TensorFlow
- Training and fine-tuning neural networks
- Working with large datasets
- Reading technical papers

## Special Notes

This is a reading-intensive course. Students are expected to read 1-2 research papers per week in addition to textbook materials. Active participation in paper discussions is a significant component of the grade.
