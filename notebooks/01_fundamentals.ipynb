{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop - Part 1: Fundamentals\n",
    "\n",
    "In this notebook, we'll explore the core concepts behind RAG:\n",
    "1. **The Problem**: Why LLMs need external knowledge\n",
    "2. **Chunking**: How to split documents for retrieval\n",
    "3. **Embeddings**: How to represent text as vectors\n",
    "4. **Vector Search**: How to find similar content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"⚠️ GOOGLE_API_KEY not found. Please set it in your .env file.\")\n",
    "else:\n",
    "    print(\"✅ API key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: LLM Hallucination\n",
    "\n",
    "Let's first see what happens when we ask an LLM about our fictional university courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "To answer your question accurately, I need to know the prerequisites for CS401 at Fictional University. Here's how we can find that information:\n",
      "\n",
      "1.  **Check the Fictional University's Course Catalog:** This is the most reliable source. Look for the official course catalog (usually available online) and search for CS401. The prerequisites will be listed in the course description.\n",
      "\n",
      "2.  **Search the Fictional University's Computer Science Department Website:** The CS department website might have a list of courses and their prerequisites.\n",
      "\n",
      "3.  **Contact the Fictional University's Computer Science Department:** If you can't find the information online, you can email or call the CS department directly. They will be able to tell you the prerequisites.\n",
      "\n",
      "**Example of what you might find:**\n",
      "\n",
      "*   **CS401: Data Structures and Algorithms**\n",
      "    *   **Prerequisites:** CS201 (Introduction to Programming) and MATH220 (Discrete Mathematics)\n",
      "\n",
      "**Without knowing the specific prerequisites listed by Fictional University, I can only give you general possibilities based on common CS curricula:**\n",
      "\n",
      "*   **Introductory Programming Course:** (e.g., CS101, CS110) - This is almost always a prerequisite.\n",
      "*   **Data Structures Course:** (e.g., CS201, CS210) - If CS401 is an advanced data structures and algorithms course, a basic data structures course is likely required.\n",
      "*   **Discrete Mathematics:** (e.g., MATH220) - This is often a prerequisite for courses involving algorithms and theoretical computer science.\n",
      "*   **Calculus:** (e.g., MATH101) - Sometimes required, especially if the course involves analysis of algorithms.\n",
      "*   **Linear Algebra:** (e.g., MATH210) - Less common than Discrete Math, but possible.\n",
      "\n",
      "**In summary, the best way to find the prerequisites for CS401 at Fictional University is to consult the official university resources (course catalog, department website, or department contact).**\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Gemini\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Ask about our fictional courses\n",
    "response = llm.invoke(\"What are the prerequisites for CS401 at Fictional University?\")\n",
    "print(\"LLM Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### LLM Control Parameters\n\n| Parameter | What it does | Values | When to use |\n|-----------|--------------|--------|-------------|\n| **temperature** | Controls randomness | 0-1 | 0-0.3 for RAG/factual, 0.7-1 for creative writing |\n| **max_output_tokens** | Limits response length | 256-4096 | Set based on expected answer length |\n| **top_p** | Nucleus sampling | 0.1-1.0 | Lower (0.1-0.5) for focused, higher for diverse |\n| **top_k** | Considers top K tokens | 1-100 | Lower for deterministic, higher for variety |\n\n**For RAG applications**: Use `temperature=0` to `0.3` for factual, consistent answers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The LLM either:\n",
    "- Admits it doesn't know (good!), or\n",
    "- Makes up an answer (hallucination - bad!)\n",
    "\n",
    "This is the fundamental problem RAG solves. Let's learn how!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Chunking: Breaking Documents into Pieces\n",
    "\n",
    "### Why do we chunk?\n",
    "1. **Context window limits**: LLMs can only process limited text at once\n",
    "2. **Retrieval granularity**: We want to retrieve relevant portions, not entire documents\n",
    "3. **Semantic coherence**: Each chunk should be a meaningful unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 4197 characters\n",
      "\n",
      "First 500 characters:\n",
      "# CS301: Introduction to Machine Learning\n",
      "\n",
      "## Course Information\n",
      "- **Course Code:** CS301\n",
      "- **Credits:** 4\n",
      "- **Prerequisites:** CS201 (Data Structures), MATH201 (Linear Algebra), STAT101 (Statistics)\n",
      "- **Instructor:** Dr. Emily Watson\n",
      "- **Office:** AI Research Center, Room 205\n",
      "- **Email:** emily.watson@fictional.edu\n",
      "- **Semester:** Fall only\n",
      "\n",
      "## Course Description\n",
      "\n",
      "This course provides a comprehensive introduction to machine learning, covering both theoretical foundations and practical applicati\n"
     ]
    }
   ],
   "source": [
    "# Load a sample document\n",
    "data_path = Path(\"../data/syllabi/CS301.md\")\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(f\"Document length: {len(document)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(document[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Strategies\n",
    "\n",
    "Let's compare different chunk sizes and see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size Comparison:\n",
      "==================================================\n",
      "\n",
      "Chunk size: 200\n",
      "  Number of chunks: 37\n",
      "  Average chunk length: 115 chars\n",
      "\n",
      "Chunk size: 500\n",
      "  Number of chunks: 12\n",
      "  Average chunk length: 352 chars\n",
      "\n",
      "Chunk size: 1000\n",
      "  Number of chunks: 5\n",
      "  Average chunk length: 841 chars\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_document(text, chunk_size, chunk_overlap):\n",
    "    \"\"\"Chunk a document and return statistics.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Compare different chunk sizes\n",
    "chunk_sizes = [200, 500, 1000]\n",
    "\n",
    "print(\"Chunk Size Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    chunks = chunk_document(document, chunk_size=size, chunk_overlap=50)\n",
    "    avg_len = sum(len(c) for c in chunks) / len(chunks)\n",
    "    print(f\"\\nChunk size: {size}\")\n",
    "    print(f\"  Number of chunks: {len(chunks)}\")\n",
    "    print(f\"  Average chunk length: {avg_len:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Look at the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Chunks (500 chars):\n",
      "==================================================\n",
      "\n",
      "--- Chunk 1 (366 chars) ---\n",
      "# CS301: Introduction to Machine Learning\n",
      "\n",
      "## Course Information\n",
      "- **Course Code:** CS301\n",
      "- **Credits:** 4\n",
      "- **Prerequisites:** CS201 (Data Structures), MATH201 (Linear Algebra), STAT101 (Statistics)\n",
      "- **Instructor:** Dr. Emily Watson\n",
      "- **Office:** AI Research Center, Room 205\n",
      "- **Email:** emily.wat...\n",
      "\n",
      "--- Chunk 2 (303 chars) ---\n",
      "## Course Description\n",
      "\n",
      "This course provides a comprehensive introduction to machine learning, covering both theoretical foundations and practical applications. Students will learn the fundamental algorithms and techniques used to build systems that can learn from data and make predictions or decisio...\n",
      "\n",
      "--- Chunk 3 (477 chars) ---\n",
      "The course bridges the gap between mathematical theory and real-world implementation. Students will gain hands-on experience with popular machine learning libraries (scikit-learn, pandas, numpy) while developing a deep understanding of the underlying algorithms.\n",
      "\n",
      "Machine learning is transforming ind...\n"
     ]
    }
   ],
   "source": [
    "# Create chunks with our chosen size\n",
    "chunks = chunk_document(document, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "print(\"Sample Chunks (500 chars):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:300] + \"...\" if len(chunk) > 300 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Trade-offs\n",
    "\n",
    "| Chunk Size | Pros | Cons |\n",
    "|------------|------|------|\n",
    "| **Small (200)** | Precise retrieval | May lose context |\n",
    "| **Medium (500)** | Good balance | Usually the sweet spot |\n",
    "| **Large (1000)** | More context | May include irrelevant info |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Embeddings: Text to Vectors\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "\n",
    "**Key insight**: Similar meanings → similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ebe7ed4d674c63ba5b9d3a687e630a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a657c477423d44b2b87bb5b6a4343ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96128c452e19401cb20befa2c13289b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be42490e4b7843c9b25dc212cb5eec41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8540830eb7b34573885bd995028bcefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5ede106994453f8c2546714a9777fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddc1f68769f4b46ad9f1f8d5930d720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d33316f4464aa1afaf9bd15b34adef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fce75ab02a3481bb7f54e8d629bc6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800f7ec399844d33bfa73609ebb9c8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87730d99d3c849c3911261c3fc495bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load embedding model (runs locally, no API needed)\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (5, 384)\n",
      "Each sentence becomes a vector of 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Let's embed some example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline rested on a rug\",\n",
    "    \"The stock market crashed yesterday\",\n",
    "    \"Machine learning is a subset of AI\",\n",
    "    \"Deep learning uses neural networks\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Each sentence becomes a vector of {embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "Let's compute similarity between sentences using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Similarity Matrix:\n",
      "============================================================\n",
      "\n",
      "[0] The cat sat on the mat\n",
      "\n",
      "[1] A feline rested on a rug\n",
      "\n",
      "[2] The stock market crashed yesterday\n",
      "\n",
      "[3] Machine learning is a subset of AI\n",
      "\n",
      "[4] Deep learning uses neural networks\n",
      "\n",
      "\n",
      "Similarity scores:\n",
      "        [0]  [1]  [2]  [3]  [4]\n",
      "[0]    1.00  0.56  0.11  -0.05  -0.08\n",
      "[1]    0.56  1.00  0.08  -0.11  -0.04\n",
      "[2]    0.11  0.08  1.00  0.05  0.04\n",
      "[3]    -0.05  -0.11  0.05  1.00  0.43\n",
      "[4]    -0.08  -0.04  0.04  0.43  1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Sentence Similarity Matrix:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Print as a readable table\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"\\n[{i}] {sent[:40]}...\" if len(sent) > 40 else f\"\\n[{i}] {sent}\")\n",
    "\n",
    "print(\"\\n\\nSimilarity scores:\")\n",
    "print(\"       \", \"  \".join([f\"[{i}]\" for i in range(len(sentences))]))\n",
    "for i, row in enumerate(similarity_matrix):\n",
    "    scores = \"  \".join([f\"{s:.2f}\" for s in row])\n",
    "    print(f\"[{i}]    {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- Sentences [0] and [1] are similar (cat/mat vs feline/rug) - high similarity!\n",
    "- Sentences [3] and [4] are similar (both about ML) - high similarity!\n",
    "- Sentence [2] (stock market) is different from the others - low similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Vector Search with ChromaDB\n",
    "\n",
    "Now let's put it together: chunk our course documents, embed them, and search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChromaDB collection created!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Create a ChromaDB client (in-memory for this demo)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Use sentence-transformers for embeddings\n",
    "embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "collection = client.create_collection(\n",
    "    name=\"course_syllabus\",\n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "print(\"✅ ChromaDB collection created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 86 chunks from 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Load all syllabi and add to collection\n",
    "syllabi_path = Path(\"../data/syllabi\")\n",
    "\n",
    "all_chunks = []\n",
    "all_ids = []\n",
    "all_metadata = []\n",
    "\n",
    "for filepath in syllabi_path.glob(\"*.md\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunks = chunk_document(content, chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    # Create IDs and metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        all_ids.append(f\"{filepath.stem}_{i}\")\n",
    "        all_metadata.append({\"source\": filepath.stem})\n",
    "\n",
    "# Add to collection\n",
    "collection.add(\n",
    "    documents=all_chunks,\n",
    "    ids=all_ids,\n",
    "    metadatas=all_metadata\n",
    ")\n",
    "\n",
    "print(f\"✅ Added {len(all_chunks)} chunks from {len(list(syllabi_path.glob('*.md')))} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What topics does the machine learning course cover?\n",
      "============================================================\n",
      "\n",
      "--- Result 1 (from CS301) ---\n",
      "The course bridges the gap between mathematical theory and real-world implementation. Students will gain hands-on experience with popular machine learning libraries (scikit-learn, pandas, numpy) while developing a deep understanding of the underlying algorithms.\n",
      "\n",
      "Machine learning is transforming ind...\n",
      "\n",
      "--- Result 2 (from CS401) ---\n",
      "# CS401: Deep Learning\n",
      "\n",
      "## Course Information\n",
      "- **Course Code:** CS401\n",
      "- **Credits:** 4\n",
      "- **Prerequisites:** CS301 (Introduction to Machine Learning)\n",
      "- **Instructor:** Dr. James Liu\n",
      "- **Office:** AI Research Center, Room 312\n",
      "- **Email:** james.liu@fictional.edu\n",
      "- **Semester:** Spring only\n",
      "\n",
      "## Course...\n",
      "\n",
      "--- Result 3 (from CS301) ---\n",
      "## Topics Covered\n",
      "\n",
      "### Module 1: Foundations (Weeks 1-2)\n",
      "- What is machine learning?\n",
      "- Types of ML: supervised, unsupervised, reinforcement\n",
      "- The ML pipeline: data collection, preprocessing, modeling, evaluation\n",
      "- Python ML ecosystem (numpy, pandas, scikit-learn)\n",
      "\n",
      "### Module 2: Supervised Learning -...\n"
     ]
    }
   ],
   "source": [
    "def search(query, n_results=3):\n",
    "    \"\"\"Search for relevant chunks.\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "query = \"What topics does the machine learning course cover?\"\n",
    "results = search(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"\\n--- Result {i+1} (from {metadata['source']}) ---\")\n",
    "    print(doc[:300] + \"...\" if len(doc) > 300 else doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who teaches linear algebra?\n",
      "============================================================\n",
      "\n",
      "--- Result 1 (from MATH201) ---\n",
      "Linear algebra provides the mathematical foundation for machine learning, computer graphics, quantum mechanics, and many other fields. The concepts learned in this course are essential for advanced study in mathematics and for practical applications in science and technology.\n",
      "\n",
      "--- Result 2 (from MATH201) ---\n",
      "## Course Description\n",
      "\n",
      "This course introduces the fundamental concepts of linear algebra, including vectors, matrices, linear transformations, and eigenvalues. Linear algebra is one of the most widely used areas of mathematics, with applications in computer science, engineering, physics, economics, ...\n",
      "\n",
      "--- Result 3 (from MATH201) ---\n",
      "## Why Linear Algebra Matters\n",
      "\n",
      "Linear algebra is essential for:\n",
      "- **Machine Learning:** Neural networks, dimensionality reduction\n",
      "- **Computer Graphics:** 3D transformations, rendering\n",
      "- **Data Science:** Principal component analysis, regression\n",
      "- **Engineering:** Signal processing, control systems\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Try another query\n",
    "query = \"Who teaches linear algebra?\"\n",
    "results = search(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"\\n--- Result {i+1} (from {metadata['source']}) ---\")\n",
    "    print(doc[:300] + \"...\" if len(doc) > 300 else doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Limitation: Relationship Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can I take CS401 if I've only completed CS101?\n",
      "============================================================\n",
      "\n",
      "--- Result 1 (from CS201) ---\n",
      "Building on the programming foundations from CS101, this course takes students deeper into how data is organized, stored, and manipulated efficiently. Understanding these concepts is essential for success in advanced computer science courses and professional software development.\n",
      "\n",
      "## Learning Object...\n",
      "\n",
      "--- Result 2 (from CS201) ---\n",
      "# CS201: Data Structures and Algorithms\n",
      "\n",
      "## Course Information\n",
      "- **Course Code:** CS201\n",
      "- **Credits:** 4\n",
      "- **Prerequisites:** CS101 (Introduction to Programming)\n",
      "- **Instructor:** Prof. Michael Torres\n",
      "- **Office:** Engineering Building, Room 415\n",
      "- **Email:** michael.torres@fictional.edu\n",
      "- **Semester...\n",
      "\n",
      "--- Result 3 (from CS401) ---\n",
      "## Prerequisites in Detail\n",
      "\n",
      "**CS301 is strictly required.** This course assumes familiarity with:\n",
      "- Machine learning fundamentals (supervised/unsupervised learning)\n",
      "- Model evaluation and validation\n",
      "- Python programming and numpy\n",
      "- Basic neural network concepts\n",
      "\n",
      "Students should also be comfortable w...\n"
     ]
    }
   ],
   "source": [
    "# This query is harder - it requires understanding relationships\n",
    "query = \"Can I take CS401 if I've only completed CS101?\"\n",
    "results = search(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"\\n--- Result {i+1} (from {metadata['source']}) ---\")\n",
    "    print(doc[:300] + \"...\" if len(doc) > 300 else doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: The search returns relevant chunks, but doesn't necessarily understand the full prerequisite chain (CS401 → CS301 → CS201 → CS101).\n",
    "\n",
    "This is where:\n",
    "- **RAG with good prompting** can help\n",
    "- **Graph RAG** excels (captures relationships explicitly)\n",
    "- **Agentic RAG** can reason through multiple steps\n",
    "\n",
    "We'll explore these in the next notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "1. **The Problem**: LLMs hallucinate when they don't have the right knowledge\n",
    "\n",
    "2. **Chunking**: Breaking documents into meaningful pieces\n",
    "   - Balance chunk size: not too small (loses context), not too large (noise)\n",
    "   - Use overlap to preserve context at boundaries\n",
    "\n",
    "3. **Embeddings**: Converting text to vectors\n",
    "   - Similar meanings → similar vectors\n",
    "   - Enable semantic search (not just keyword matching)\n",
    "\n",
    "4. **Vector Search**: Finding relevant content\n",
    "   - ChromaDB stores and searches embeddings\n",
    "   - Returns top-k most similar chunks\n",
    "\n",
    "**Next**: In notebook 02, we'll build a complete RAG pipeline that combines retrieval with LLM generation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}